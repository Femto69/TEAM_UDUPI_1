# -*- coding: utf-8 -*-
"""XGB aqi preidiction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1U0c3S_hh61kydVKEmNsMoPyLV7HkTPzP
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score, GridSearchCV
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error as mae
from sklearn.metrics import mean_squared_error as mse

data = pd.read_csv('/content/data.csv')

data.head()

data.info()

data.drop(['Unnamed: 0'],axis=1,inplace=True)

data.head()

data.columns= ['avg temp','max temp','min temp','pressure','humidity','visibility' ,'avg wind speed','max wind speed' , 'aqi']

data.head()

mean_value = data['aqi'].mean()

data['aqi'] = data['aqi'].fillna(mean_value)

data.info()

X = data.drop(['aqi'],axis=1)
Y = data['aqi']

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=3)

models = [RandomForestRegressor(), DecisionTreeRegressor(), XGBRegressor()]
def compare_models_cross_validation():
  for model in models:
    cv_score = cross_val_score(model, X, Y, scoring='neg_mean_absolute_error', cv=5)
    mean_mae = -cv_score.mean()
    print('mae for', model, 'is', mean_mae)

compare_models_cross_validation()

models = [RandomForestRegressor(),  XGBRegressor()]
model_hyperparameters = {
    'rf_hp': {'n_estimators': [100, 200, 300, 500],
              'max_features': ['auto', 'sqrt', 'log2'],
              'max_depth': [None, 10, 20, 30],
              'min_samples_split': [2, 5, 10],
              'min_samples_leaf': [1, 2, 4],
              'bootstrap': [True, False] ,
              'criterion': ['squared_error', 'absolute_error']},
   'xgbR_hp': {'n_estimators': [100, 200, 300, 500],
                 'max_depth': [3, 6, 9],
                 'learning_rate': [0.01, 0.1, 0.3],
                 'subsample': [0.6, 0.8, 1.0],
                 'colsample_bytree': [0.6, 0.8, 1.0],
                'gamma': [0, 0.1, 0.3]}

}

model_keys = list(model_hyperparameters.keys())

model_hyperparameters[model_keys[0]]

def model_selection(list_of_models, hyperparameters):
  result= []
  i = 0
  for model in list_of_models:
    key = model_keys[i]
    params = hyperparameters[key]
    i = i + 1
    print(model)
    print(params)
    print('--------------------------------------')

    regression_ = GridSearchCV(estimator=model, param_grid=params, cv=5, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2)
    regression_.fit(X, Y)
    result.append({
        'model used': model,
        'highest score': regression_.best_score_,
        'best hyperparameters': regression_.best_params_
    })

model_selection(models,model_hyperparameters )

from sklearn.model_selection import RandomizedSearchCV

def model_selection_(list_of_models, hyperparameters):
    result = []
    i = 0
    for model in list_of_models:
        key = list(hyperparameters.keys())[i]  # Updated to fix the key selection
        params = hyperparameters[key]
        i += 1
        print(model)
        print(params)
        print('--------------------------------------')

        regression_ = RandomizedSearchCV(estimator=model, param_distributions=params, n_iter=100, cv=5, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2, random_state=42)
        regression_.fit(X, Y)
        result.append({
            'model used': model,
            'highest score': regression_.best_score_,
            'best hyperparameters': regression_.best_params_
        })
    return result
  model_selection_(models,model_hyperparameters )

