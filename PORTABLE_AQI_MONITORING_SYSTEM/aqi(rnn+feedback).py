# -*- coding: utf-8 -*-
"""aqi(rnn+feedback).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16aCJw-CW6TJjUMY8kVhWDR4dJsCA9QvZ
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
import pandas as pd
from sklearn.metrics import mean_squared_error, mean_absolute_error
from torch.utils.data import DataLoader, TensorDataset

# Load the dataset
df = pd.read_csv('/content/delhi_aqi.csv')

# Convert 'date' to datetime and set it as the index
df['date'] = pd.to_datetime(df['date'])
df.set_index('date', inplace=True)

# List of feature columns (excluding the target)
feature_columns = ['co', 'no', 'no2', 'o3', 'so2', 'pm2_5', 'pm10', 'nh3']

# Normalize the features (optional but recommended for training)
df[feature_columns] = (df[feature_columns] - df[feature_columns].min()) / (df[feature_columns].max() - df[feature_columns].min())

# Select the target variable (e.g., 'pm2_5' as a proxy for AQI)
target_column = 'pm2_5'

# Create sequences of data for training
def create_sequences(features, target, seq_length):
    xs, ys = [], []
    for i in range(len(features) - seq_length):
        x = features[i:i + seq_length]
        y = target[i + seq_length]
        xs.append(x)
        ys.append(y)
    return np.array(xs), np.array(ys)

SEQ_LENGTH = 10  # Length of the sequence to be used for prediction
X, y = create_sequences(df[feature_columns].values, df[target_column].values, SEQ_LENGTH)

# Convert data to PyTorch tensors
X = torch.tensor(X, dtype=torch.float32)
y = torch.tensor(y, dtype=torch.float32).unsqueeze(-1)

# Create a DataLoader
dataset = TensorDataset(X, y)
train_loader = DataLoader(dataset, batch_size=32, shuffle=True)

class AQIPredictorRNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(AQIPredictorRNN, self).__init__()
        self.hidden_size = hidden_size
        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, input_seq, hidden):
        # Forward pass through RNN
        out, hidden = self.rnn(input_seq, hidden)

        # Pass the output through a fully connected layer
        out = self.fc(out[:, -1, :])  # Use the last output in the sequence

        # Feedback: use the output as part of the next input (if needed in loop)
        feedback = out.unsqueeze(1)

        return out, feedback, hidden

    def init_hidden(self, batch_size):
        # Initialize hidden state with zeros
        return torch.zeros(1, batch_size, self.hidden_size)

# Initialize model parameters
INPUT_SIZE = len(feature_columns)  # Number of input features
HIDDEN_SIZE = 64
OUTPUT_SIZE = 1  # We're predicting a single value (e.g., 'pm2_5')

model = AQIPredictorRNN(INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE)

# Define the loss function and optimizer
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training loop
EPOCHS = 100

for epoch in range(EPOCHS):
    for batch_idx, (inputs, targets) in enumerate(train_loader):
        # Initialize the hidden state
        hidden = model.init_hidden(inputs.size(0))

        # Forward pass
        outputs, feedback, hidden = model(inputs, hidden)

        # Compute loss
        loss = criterion(outputs, targets)

        # Backpropagation and optimization
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    # Print loss every epoch
    print(f'Epoch [{epoch+1}/{EPOCHS}], Loss: {loss.item():.4f}')

# Save the trained model
torch.save(model.state_dict(), 'aqi_predictor_rnn.pth')

# Example of how to evaluate the model on a single sequence
model.eval()  # Set the model to evaluation mode
with torch.no_grad():
    test_seq = X[0].unsqueeze(0)  # Example sequence
    hidden = model.init_hidden(1)
    output, _, _ = model(test_seq, hidden)
    print(f'Predicted pm2.5: {output.item()}, True pm2.5: {y[0].item()}')

# Assuming 'pm2_5' has been normalized to [0, 1]
# Get the min and max values from the original dataset
pm2_5_min = df['pm2_5'].min()
pm2_5_max = df['pm2_5'].max()

# Denormal
print(pm2_5_min, pm2_5_max)
print(output.item() * (pm2_5_max - pm2_5_min) + pm2_5_min)

def denormalize(value, min_val, max_val):
    return value * (max_val - min_val) + min_val

predictions_denorm = denormalize(output.item(), pm2_5_min, pm2_5_max)
true_values_denorm = denormalize(, pm2_5_min, pm2_5_max)

# Calculate Mean Squared Error and Mean Absolute Error
mse = mean_squared_error(true_values_denorm, predictions_denorm)
mae = mean_absolute_error(true_values_denorm, predictions_denorm)

print(f'Mean Squared Error: {mse:.4f}')
print(f'Mean Absolute Error: {mae:.4f}')

